{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53dd9f2-80ba-44c7-b5a5-5afe3cfe0888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "import implicit\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 最大表示列数の指定（ここでは50列を指定）\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "from hydra import compose, initialize\n",
    "\n",
    "from utils import load_datasets\n",
    "from utils.embedding import TextEmbedder\n",
    "\n",
    "with initialize(config_path=\"../yamls\", version_base=None):\n",
    "    config = compose(config_name=\"config.yaml\")\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(Path(config.input_path) / \"train.csv\")\n",
    "test_df = pd.read_csv(Path(config.input_path) / \"test.csv\")\n",
    "\n",
    "sample_submission_df = pd.read_csv(Path(config.input_path) / \"sample_submission.csv\")\n",
    "anime_df = pd.read_csv(Path(config.input_path) / \"anime.csv\")\n",
    "\n",
    "# 整形\n",
    "anime_df[\"genres\"] = anime_df[\"genres\"].str.replace(\" \", \"\")\n",
    "\n",
    "# Merge the train data with the anime meta data\n",
    "all_df = pd.concat([train_df, test_df])\n",
    "all_df = all_df.merge(anime_df, on=\"anime_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "202d46f5-9048-4411-9878-6b0c03cc6c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, ModuleList\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.conv import LGConv\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "from torch_geometric.utils import is_sparse, to_edge_index\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# make Data\n",
    "all_df = pd.concat([train_df[[\"user_id\", \"anime_id\"]], test_df[[\"user_id\", \"anime_id\"]]]).reset_index(drop=True)\n",
    "all_df[\"user_label\"], user_idx = pd.factorize(all_df[\"user_id\"])\n",
    "all_df[\"anime_label\"], anime_idx = pd.factorize(all_df[\"anime_id\"])\n",
    "all_df[\"is_train\"] = True\n",
    "all_df.loc[len(train_df) :, \"is_train\"] = False\n",
    "# userとanimeの番号が別になるようにずらす\n",
    "all_df[\"anime_label\"] += len(user_idx)\n",
    "num_nodes = len(user_idx) + len(anime_idx)\n",
    "edges = all_df[[\"user_label\", \"anime_label\"]].to_numpy()\n",
    "edge_index = torch.tensor(edges.T, dtype=torch.long).contiguous()\n",
    "data = Data(num_nodes=num_nodes, edge_index=edge_index).to(device)\n",
    "data.edge_weight = torch.ones(len(all_df)).contiguous().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a9c08dc-9d26-48fc-8a71-215133888486",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 254077], num_nodes=3954, edge_weight=[254077])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39fccebd-3f18-4f2e-ba51-bb32351dc147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import RandomLinkSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a46440-2736-4072-a621-9e3b9ebfb37c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 254077], num_nodes=3954, edge_weight=[254077], edge_label=[508154], edge_label_index=[2, 508154])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = RandomLinkSplit(num_val=0, num_test=0, add_negative_train_samples=True, neg_sampling_ratio=1.0)\n",
    "train_data, _, _ = transform(data)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e004faaf-1742-48ae-9663-ad508923b3b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1834, 1906, 1112,  ..., 3381, 3163,   37],\n",
       "        [3886, 2277, 2320,  ...,   61,  478, 3721]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.edge_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "476ae819-bc70-4433-9143-466b5acb5230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding, ModuleList\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch_geometric.nn.conv import LGConv\n",
    "from torch_geometric.typing import Adj, OptTensor\n",
    "from torch_geometric.utils import is_sparse, to_edge_index\n",
    "\n",
    "\n",
    "class LightGCN(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int,\n",
    "        embedding_dim: int,\n",
    "        num_layers: int,\n",
    "        alpha: Optional[Union[float, Tensor]] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        if alpha is None:\n",
    "            alpha = 1.0 / (num_layers + 1)\n",
    "\n",
    "        if isinstance(alpha, Tensor):\n",
    "            assert alpha.size(0) == num_layers + 1\n",
    "        else:\n",
    "            alpha = torch.tensor([alpha] * (num_layers + 1))\n",
    "        self.register_buffer(\"alpha\", alpha)\n",
    "\n",
    "        self.embedding = Embedding(num_nodes, embedding_dim)\n",
    "        self.convs = ModuleList([LGConv(**kwargs) for _ in range(num_layers)])\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        torch.nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def get_embedding(\n",
    "        self,\n",
    "        edge_index: Adj,\n",
    "        edge_weight: OptTensor = None,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Returns the embedding of nodes in the graph.\"\"\"\n",
    "        x = self.embedding.weight\n",
    "        out = x * self.alpha[0]\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index, edge_weight)\n",
    "            out = out + x * self.alpha[i + 1]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        edge_index: Adj,\n",
    "        edge_label_index: OptTensor = None,\n",
    "        edge_weight: OptTensor = None,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Computes rankings for pairs of nodes.\n",
    "\n",
    "        Args:\n",
    "            edge_index (torch.Tensor or SparseTensor): Edge tensor specifying\n",
    "                the connectivity of the graph.\n",
    "            edge_label_index (torch.Tensor, optional): Edge tensor specifying\n",
    "                the node pairs for which to compute rankings or probabilities.\n",
    "                If :obj:`edge_label_index` is set to :obj:`None`, all edges in\n",
    "                :obj:`edge_index` will be used instead. (default: :obj:`None`)\n",
    "            edge_weight (torch.Tensor, optional): The weight of each edge in\n",
    "                :obj:`edge_index`. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        if edge_label_index is None:\n",
    "            if is_sparse(edge_index):\n",
    "                edge_label_index, _ = to_edge_index(edge_index)\n",
    "            else:\n",
    "                edge_label_index = edge_index\n",
    "\n",
    "        out = self.get_embedding(edge_index, edge_weight)\n",
    "\n",
    "        out_src = out[edge_label_index[0]]\n",
    "        out_dst = out[edge_label_index[1]]\n",
    "\n",
    "        return (out_src * out_dst).sum(dim=-1)\n",
    "\n",
    "    def predict_link(\n",
    "        self,\n",
    "        edge_index: Adj,\n",
    "        edge_label_index: OptTensor = None,\n",
    "        edge_weight: OptTensor = None,\n",
    "        prob: bool = False,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Predict links between nodes specified in :obj:`edge_label_index`.\n",
    "\n",
    "        Args:\n",
    "            prob (bool, optional): Whether probabilities should be returned.\n",
    "                (default: :obj:`False`)\n",
    "        \"\"\"\n",
    "        pred = self(edge_index, edge_label_index, edge_weight).sigmoid()\n",
    "        return pred if prob else pred.round()\n",
    "\n",
    "    def recommend(\n",
    "        self,\n",
    "        edge_index: Adj,\n",
    "        edge_weight: OptTensor = None,\n",
    "        src_index: OptTensor = None,\n",
    "        dst_index: OptTensor = None,\n",
    "        k: int = 1,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Get top-:math:`k` recommendations for nodes in :obj:`src_index`.\n",
    "\n",
    "        Args:\n",
    "            src_index (torch.Tensor, optional): Node indices for which\n",
    "                recommendations should be generated.\n",
    "                If set to :obj:`None`, all nodes will be used.\n",
    "                (default: :obj:`None`)\n",
    "            dst_index (torch.Tensor, optional): Node indices which represent\n",
    "                the possible recommendation choices.\n",
    "                If set to :obj:`None`, all nodes will be used.\n",
    "                (default: :obj:`None`)\n",
    "            k (int, optional): Number of recommendations. (default: :obj:`1`)\n",
    "        \"\"\"\n",
    "        out_src = out_dst = self.get_embedding(edge_index, edge_weight)\n",
    "\n",
    "        if src_index is not None:\n",
    "            out_src = out_src[src_index]\n",
    "\n",
    "        if dst_index is not None:\n",
    "            out_dst = out_dst[dst_index]\n",
    "\n",
    "        pred = out_src @ out_dst.t()\n",
    "        top_index = pred.topk(k, dim=-1).indices\n",
    "\n",
    "        if dst_index is not None:  # Map local top-indices to original indices.\n",
    "            top_index = dst_index[top_index.view(-1)].view(*top_index.size())\n",
    "\n",
    "        return top_index\n",
    "\n",
    "    def link_pred_loss(self, pred: Tensor, edge_label: Tensor, **kwargs) -> Tensor:\n",
    "        r\"\"\"Computes the model loss for a link prediction objective via the\n",
    "        :class:`torch.nn.BCEWithLogitsLoss`.\n",
    "\n",
    "        Args:\n",
    "            pred (torch.Tensor): The predictions.\n",
    "            edge_label (torch.Tensor): The ground-truth edge labels.\n",
    "            **kwargs (optional): Additional arguments of the underlying\n",
    "                :class:`torch.nn.BCEWithLogitsLoss` loss function.\n",
    "        \"\"\"\n",
    "        loss_fn = torch.nn.BCEWithLogitsLoss(**kwargs)\n",
    "        return loss_fn(pred, edge_label.to(pred.dtype))\n",
    "\n",
    "    def recommendation_loss(\n",
    "        self,\n",
    "        pos_edge_rank: Tensor,\n",
    "        neg_edge_rank: Tensor,\n",
    "        node_id: Optional[Tensor] = None,\n",
    "        lambda_reg: float = 1e-4,\n",
    "        **kwargs,\n",
    "    ) -> Tensor:\n",
    "        r\"\"\"Computes the model loss for a ranking objective via the Bayesian\n",
    "        Personalized Ranking (BPR) loss.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            The i-th entry in the :obj:`pos_edge_rank` vector and i-th entry\n",
    "            in the :obj:`neg_edge_rank` entry must correspond to ranks of\n",
    "            positive and negative edges of the same entity (*e.g.*, user).\n",
    "\n",
    "        Args:\n",
    "            pos_edge_rank (torch.Tensor): Positive edge rankings.\n",
    "            neg_edge_rank (torch.Tensor): Negative edge rankings.\n",
    "            node_id (torch.Tensor): The indices of the nodes involved for\n",
    "                deriving a prediction for both positive and negative edges.\n",
    "                If set to :obj:`None`, all nodes will be used.\n",
    "            lambda_reg (int, optional): The :math:`L_2` regularization strength\n",
    "                of the Bayesian Personalized Ranking (BPR) loss.\n",
    "                (default: :obj:`1e-4`)\n",
    "            **kwargs (optional): Additional arguments of the underlying\n",
    "                :class:`torch_geometric.nn.models.lightgcn.BPRLoss` loss\n",
    "                function.\n",
    "        \"\"\"\n",
    "        loss_fn = BPRLoss(lambda_reg, **kwargs)\n",
    "        emb = self.embedding.weight\n",
    "        emb = emb if node_id is None else emb[node_id]\n",
    "        return loss_fn(pos_edge_rank, neg_edge_rank, emb)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}({self.num_nodes}, \" f\"{self.embedding_dim}, num_layers={self.num_layers})\"\n",
    "\n",
    "\n",
    "class BPRLoss(_Loss):\n",
    "    r\"\"\"The Bayesian Personalized Ranking (BPR) loss.\n",
    "\n",
    "    The BPR loss is a pairwise loss that encourages the prediction of an\n",
    "    observed entry to be higher than its unobserved counterparts\n",
    "    (see `here <https://arxiv.org/abs/2002.02126>`__).\n",
    "\n",
    "    .. math::\n",
    "        L_{\\text{BPR}} = - \\sum_{u=1}^{M} \\sum_{i \\in \\mathcal{N}_u}\n",
    "        \\sum_{j \\not\\in \\mathcal{N}_u} \\ln \\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})\n",
    "        + \\lambda \\vert\\vert \\textbf{x}^{(0)} \\vert\\vert^2\n",
    "\n",
    "    where :math:`lambda` controls the :math:`L_2` regularization strength.\n",
    "    We compute the mean BPR loss for simplicity.\n",
    "\n",
    "    Args:\n",
    "        lambda_reg (float, optional): The :math:`L_2` regularization strength\n",
    "            (default: 0).\n",
    "        **kwargs (optional): Additional arguments of the underlying\n",
    "            :class:`torch.nn.modules.loss._Loss` class.\n",
    "    \"\"\"\n",
    "    __constants__ = [\"lambda_reg\"]\n",
    "    lambda_reg: float\n",
    "\n",
    "    def __init__(self, lambda_reg: float = 0, **kwargs):\n",
    "        super().__init__(None, None, \"sum\", **kwargs)\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def forward(self, positives: Tensor, negatives: Tensor, parameters: Tensor = None) -> Tensor:\n",
    "        r\"\"\"Compute the mean Bayesian Personalized Ranking (BPR) loss.\n",
    "\n",
    "        .. note::\n",
    "\n",
    "            The i-th entry in the :obj:`positives` vector and i-th entry\n",
    "            in the :obj:`negatives` entry should correspond to the same\n",
    "            entity (*.e.g*, user), as the BPR is a personalized ranking loss.\n",
    "\n",
    "        Args:\n",
    "            positives (Tensor): The vector of positive-pair rankings.\n",
    "            negatives (Tensor): The vector of negative-pair rankings.\n",
    "            parameters (Tensor, optional): The tensor of parameters which\n",
    "                should be used for :math:`L_2` regularization\n",
    "                (default: :obj:`None`).\n",
    "        \"\"\"\n",
    "        log_prob = F.logsigmoid(positives - negatives).mean()\n",
    "\n",
    "        regularization = 0\n",
    "        if self.lambda_reg != 0:\n",
    "            regularization = self.lambda_reg * parameters.norm(p=2).pow(2)\n",
    "            regularization = regularization / positives.size(0)\n",
    "\n",
    "        return -log_prob + regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2df151c6-eee9-485d-ad30-22e63562c044",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253df623649e48f881e9cb2e159d0102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.94 GiB (GPU 0; 14.56 GiB total capacity; 5.89 GiB already allocated; 689.50 MiB free; 7.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mlr)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mnum_epochs \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m6\u001b[39m)):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_link\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_label_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlink_pred_loss(pred, train_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_label\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[9], line 107\u001b[0m, in \u001b[0;36mLightGCN.predict_link\u001b[0;34m(self, edge_index, edge_label_index, edge_weight, prob)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_link\u001b[39m(\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     96\u001b[0m     edge_index: Adj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m     prob: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Predict links between nodes specified in :obj:`edge_label_index`.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m        prob (bool, optional): Whether probabilities should be returned.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m            (default: :obj:`False`)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_label_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msigmoid()\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pred \u001b[38;5;28;01mif\u001b[39;00m prob \u001b[38;5;28;01melse\u001b[39;00m pred\u001b[38;5;241m.\u001b[39mround()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[9], line 90\u001b[0m, in \u001b[0;36mLightGCN.forward\u001b[0;34m(self, edge_index, edge_label_index, edge_weight)\u001b[0m\n\u001b[1;32m     87\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_embedding(edge_index, edge_weight)\n\u001b[1;32m     89\u001b[0m out_src \u001b[38;5;241m=\u001b[39m out[edge_label_index[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m---> 90\u001b[0m out_dst \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[43medge_label_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (out_src \u001b[38;5;241m*\u001b[39m out_dst)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.94 GiB (GPU 0; 14.56 GiB total capacity; 5.89 GiB already allocated; 689.50 MiB free; 7.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "oof_pred = np.zeros(len(train_df))\n",
    "test_preds = []\n",
    "\n",
    "model = LightGCN(\n",
    "    num_nodes=data.num_nodes,\n",
    "    embedding_dim=config.train.embedding_dim,\n",
    "    num_layers=config.train.num_layers,\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.train.lr)\n",
    "\n",
    "for epoch in tqdm(range(config.train.num_epochs if config.debug is False else 6)):\n",
    "    # train\n",
    "    pred = model.predict_link(\n",
    "        train_data.edge_index, train_data.edge_label_index, edge_weight=train_data.edge_weight, prob=True\n",
    "    )\n",
    "    loss = model.link_pred_loss(pred, train_data[\"edge_label\"])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"epoch {epoch} : loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180aa38d-35cb-4bd9-90ca-132f2ed2593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
