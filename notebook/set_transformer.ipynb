{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ff95c51d-faf3-4203-8311-c89a0dfbcaeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MAB(nn.Module):\n",
    "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False):\n",
    "        super(MAB, self).__init__()\n",
    "        self.dim_V = dim_V\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
    "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
    "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
    "        if ln:\n",
    "            self.ln0 = nn.LayerNorm(dim_V)\n",
    "            self.ln1 = nn.LayerNorm(dim_V)\n",
    "        self.fc_o = nn.Linear(dim_V, dim_V)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        Q = self.fc_q(Q)\n",
    "        K, V = self.fc_k(K), self.fc_v(K)\n",
    "\n",
    "        dim_split = self.dim_V // self.num_heads\n",
    "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
    "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
    "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
    "\n",
    "        A = torch.softmax(Q_.bmm(K_.transpose(1, 2)) / math.sqrt(self.dim_V), 2)\n",
    "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
    "        O = O if getattr(self, \"ln0\", None) is None else self.ln0(O)\n",
    "        O = O + F.relu(self.fc_o(O))\n",
    "        O = O if getattr(self, \"ln1\", None) is None else self.ln1(O)\n",
    "        return O\n",
    "\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, ln=False):\n",
    "        super(SAB, self).__init__()\n",
    "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mab(X, X)\n",
    "\n",
    "\n",
    "class ISAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False):\n",
    "        super(ISAB, self).__init__()\n",
    "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
    "        nn.init.xavier_uniform_(self.I)\n",
    "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln)\n",
    "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X)\n",
    "        return self.mab1(X, H)\n",
    "\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds, ln=False):\n",
    "        super(PMA, self).__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mab(self.S.repeat(X.size(0), 1, 1), X)\n",
    "\n",
    "\n",
    "class SetTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_animes, embed_dim, dim_output, num_layers=3, num_inds=32, dim_hidden=128, num_heads=4, ln=False\n",
    "    ):\n",
    "        super(SetTransformer, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_animes + 1, embed_dim)  # num_animes+1 は穴埋め用データ\n",
    "\n",
    "        layers = []\n",
    "        layers.append(ISAB(embed_dim, dim_hidden, num_heads, num_inds, ln=ln))\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln))\n",
    "        layers.append(nn.Linear(dim_hidden, 1))\n",
    "        self.enc = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X_embed = self.embedding(X)  # Embed anime_id\n",
    "        return self.enc(X_embed).squeeze(2)  # Return shape: [batch_size, num_items, dim_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4539b65c-6dc7-4255-99dc-c15934eb28c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "import implicit\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from scipy.sparse import csr_matrix, random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# 最大表示列数の指定（ここでは50列を指定）\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "from hydra import compose, initialize\n",
    "\n",
    "from utils import load_datasets\n",
    "from utils.embedding import TextEmbedder\n",
    "\n",
    "with initialize(config_path=\"../yamls\", version_base=None):\n",
    "    config = compose(config_name=\"config.yaml\")\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(Path(config.input_path) / \"train.csv\")\n",
    "test_df = pd.read_csv(Path(config.input_path) / \"test.csv\")\n",
    "all_df = pd.concat([train_df, test_df])\n",
    "all_df[\"anime_label\"], anime_idx = pd.factorize(all_df[\"anime_id\"])\n",
    "train_df[\"anime_label\"] = all_df[: len(train_df)][\"anime_label\"]\n",
    "test_df[\"anime_label\"] = all_df[len(train_df) :][\"anime_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3e8f3880-a61e-49ee-896b-1bf9974751ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.768770023680179"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6ac390c4-91e6-4158-bbcc-2eec5d2f2ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "25b08983-29b8-44ef-995c-55dd958d8338",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 start !\n",
      "Train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b23ecc5bff426788f8499fa23d9eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb30117c9ede4c6eb68a3a879ac36b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0: Training Loss 5.339135652927799,  Validation Loss 3.615911900683899\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420936b3e5524d07af928b0816a2ee2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc33bf52e014632b84c5391e02bf928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1: Training Loss 3.4186819352556443,  Validation Loss 1.678759555012833\n",
      "Fold 1 start !\n",
      "Train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7625b4151164a68a089698a5b237bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f67c05ce3cb4b62920a3f26d8908e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0: Training Loss 4.901848770588641,  Validation Loss 2.0002497565485156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bbb7e0c7234bed97ba0b54ef2418e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bd2f5c10cd447da846bcba5aec3ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1: Training Loss 1.7223258653821398,  Validation Loss 1.7764008017736426\n",
      "Fold 2 start !\n",
      "Train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2d9499f1214e36b227739f6dcfb3f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988d9486e2124dc49c5ade91a0e947c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0: Training Loss 5.296896346630629,  Validation Loss 2.1614353258638235\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0d1817935746ee8919ddbc5c378d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d601f411ad6f45269178b3d88e502dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1: Training Loss 1.919488730874751,  Validation Loss 1.604829675877223\n",
      "Fold 3 start !\n",
      "Train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90421a11fb504ae1bc67290f1b09b285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c654efaf6c8b40388bf1f203fe729ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0: Training Loss 5.250569074617688,  Validation Loss 1.8972786525653045\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e79dfb4c6f04092ba462d02d2898b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47982c3618d043948367449a2df087f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1: Training Loss 1.9567877773405296,  Validation Loss 2.55394148332572\n",
      "Fold 4 start !\n",
      "Train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d89146d85c4a3eb705f1b37ec236f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7202ad34e9664e31babcfb7239a23d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0: Training Loss 4.585021445104698,  Validation Loss 3.1258762422785056\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cda95e6249446ac9b9aa212990b832e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ea98ed44ec4841b831d4c9ca77225f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1: Training Loss 2.1479635831288832,  Validation Loss 1.893505764666031\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "output_path = Path(f\".\")\n",
    "device = \"cpu\"  # \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_df = train_df.head(10000)\n",
    "test_df = test_df.head(2000)\n",
    "\n",
    "\n",
    "oof_pred = np.zeros(train_df.shape[0])\n",
    "test_preds_all = []\n",
    "\n",
    "test_grouped_anime = test_df.groupby(\"user_id\")[\"anime_label\"].apply(list)\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "kf = StratifiedGroupKFold(n_splits=config.nn.num_folds, shuffle=True, random_state=config.seed)\n",
    "for fold, (train_index, valid_index) in enumerate(kf.split(train_df, train_df[\"score\"], train_df[\"user_id\"])):\n",
    "    print(f\"Fold {fold} start !\")\n",
    "    # user_idごとにanime_idのリストと平均スコアを取得\n",
    "    train_grouped_anime = train_df.iloc[train_index].groupby(\"user_id\")[\"anime_label\"].apply(list)\n",
    "    train_grouped_score = train_df.iloc[train_index].groupby(\"user_id\")[\"score\"].apply(list)\n",
    "    valid_grouped_anime = train_df.iloc[valid_index].groupby(\"user_id\")[\"anime_label\"].apply(list)\n",
    "    valid_grouped_score = train_df.iloc[valid_index].groupby(\"user_id\")[\"score\"].apply(list)\n",
    "    test_grouped_anime = test_df.groupby(\"user_id\")[\"anime_label\"].apply(list)\n",
    "\n",
    "    train_dataset = AnimeDataset(num_animes, train_grouped_anime, train_grouped_score, num_samples)\n",
    "    valid_dataset = AnimeDataset(num_animes, valid_grouped_anime, valid_grouped_score, num_samples)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = SetTransformer(\n",
    "        num_animes=num_animes,\n",
    "        embed_dim=embed_dim,\n",
    "        dim_output=dim_output,  # Since we want to predict scores, the dim_output is 1.\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()  # Using MSE as the loss for RMSE\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    print(\"Train\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, (data, scores) in enumerate(tqdm(train_loader)):\n",
    "            data, scores = data.to(device), scores.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = torch.sqrt(criterion(outputs, scores))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() ** 2\n",
    "\n",
    "        # Validate the model\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (data, scores) in enumerate(tqdm(valid_loader)):\n",
    "                data, scores = data.to(device), scores.to(device)\n",
    "                outputs = model(data)\n",
    "                loss = torch.sqrt(criterion(outputs.squeeze(), scores))\n",
    "                valid_loss += loss.item() ** 2\n",
    "\n",
    "        train_loss = np.sqrt(train_loss / len(train_loader))\n",
    "        valid_loss = np.sqrt(valid_loss / len(valid_loader))\n",
    "        print(f\"Epoch{epoch}: Training Loss {train_loss},  Validation Loss {valid_loss}\")\n",
    "        \"\"\"\n",
    "        wandb.log(\n",
    "            {\"epoch\": epoch, f\"nn/train_loss/fold-{fold}\": train_loss, f\"nn/valid_loss/fold-{fold}\": valid_loss}\n",
    "        )\n",
    "        \"\"\"\n",
    "        # Save the model if it has the best score so far\n",
    "        if valid_loss < best_val_loss:\n",
    "            torch.save(model.state_dict(), f\"{output_path}/best_model.pt\")\n",
    "            best_val_loss = valid_loss\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= config.nn.early_stopping:\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(f\"{output_path}/best_model.pt\"))\n",
    "    model.eval()\n",
    "    valid_dataset = AnimeDataset(num_animes, valid_grouped_anime, valid_grouped_score, None)\n",
    "    test_dataset = AnimeDataset(num_animes, test_grouped_anime, None, None)\n",
    "    valid_preds = []\n",
    "    test_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(valid_dataset)):\n",
    "            valid_pred = []\n",
    "            for itts in range(num_tts):  # indexをシャッフルしてn個ずつに分割して推論を繰り返す\n",
    "                data, _ = valid_dataset[idx]\n",
    "                data_len = len(data)\n",
    "                one_time_pred = np.zeros(len(data))\n",
    "                data, scores = data.to(device), scores.to(device)\n",
    "\n",
    "                if data_len <= num_samples:  # １回でOK\n",
    "                    one_time_pred = model(data.unsqueeze(dim=0)).numpy().flatten()[:data_len]\n",
    "                    valid_pred.append(one_time_pred)\n",
    "                    break\n",
    "                else:\n",
    "                    chunk_index_list = make_chunk_index_list(len(data), num_samples)\n",
    "                    aggregated_predictions = []\n",
    "                    for chunk_index in chunk_index_list:\n",
    "                        one_time_pred[chunk_index] = model(data[chunk_index].unsqueeze(dim=0)).numpy().flatten()\n",
    "                    valid_pred.append(one_time_pred)\n",
    "            valid_preds.append(np.mean(valid_pred, axis=0))\n",
    "\n",
    "        for idx in range(len(test_dataset)):\n",
    "            test_pred = []\n",
    "            for itts in range(num_tts):  # indexをシャッフルしてn個ずつに分割して推論を繰り返す\n",
    "                data = test_dataset[idx]\n",
    "                data_len = len(data)\n",
    "                one_time_pred = np.zeros(len(data))\n",
    "                data, scores = data.to(device), scores.to(device)\n",
    "\n",
    "                if data_len <= num_samples:  # １回でOK\n",
    "                    one_time_pred = model(data.unsqueeze(dim=0)).numpy().flatten()[:data_len]\n",
    "                    test_pred.append(one_time_pred)\n",
    "                    break\n",
    "                else:\n",
    "                    chunk_index_list = make_chunk_index_list(len(data), num_samples)\n",
    "                    aggregated_predictions = []\n",
    "\n",
    "                    for chunk_index in chunk_index_list:\n",
    "                        one_time_pred[chunk_index] = model(data[chunk_index].unsqueeze(dim=0)).numpy().flatten()\n",
    "\n",
    "                    test_pred.append(one_time_pred)\n",
    "            test_preds.append(np.mean(test_pred, axis=0))\n",
    "\n",
    "    oof_pred[valid_index] = np.concatenate(valid_preds)  # group 化してあるものを 1d に戻す\n",
    "    test_preds_all.append(np.concatenate(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f3c3a376-9487-4620-88bf-c06b3f80edcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.81555963, 7.469069  , 8.48040056, ..., 6.66080594, 7.49263716,\n",
       "       7.11457157])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00198c33-7ae0-4e6b-808a-0e748b6b1978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
