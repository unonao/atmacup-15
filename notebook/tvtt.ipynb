{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47bff823-c44c-4c8a-b458-c456a989ae2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "import implicit\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hydra import compose, initialize\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 最大表示列数の指定（ここでは50列を指定）\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "from utils import evaluate_score, load_datasets, load_sample_sub, load_target\n",
    "from utils.embedding import TextEmbedder\n",
    "\n",
    "with initialize(config_path=\"../yamls\", version_base=None):\n",
    "    config = compose(config_name=\"config.yaml\")\n",
    "config.debug = True\n",
    "\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff141ab4-3955-4a54-b607-72b76a80208c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df69d7d1-841e-43de-97bf-8d8e5e722bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from glob import glob\n",
    "from typing import Any, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "from timm.utils import AverageMeter\n",
    "from torch import nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class UserDataset(Dataset):\n",
    "    def __init__(self, merge_df: pd.DataFrame, max_padding: int = 531):\n",
    "        \"\"\"\n",
    "        merge_df: すべてのデータを結合したもの。以下のカラムを持つ。\n",
    "        - user_label: 0-indexed にした user_id\n",
    "        - anime_label: 0-indexed にした anime_id\n",
    "        - mode: その行について trainは1, validationは2, testは3 にしたもの\n",
    "        - score: testに関しては適当な値(0)で良い\n",
    "        \"\"\"\n",
    "        self.merge_df = merge_df\n",
    "        self.max_padding = max_padding\n",
    "        self.user2anime_dict = merge_df.groupby(\"user_label\")[\"anime_label\"].apply(list).to_dict()\n",
    "        self.user2mode_dict = merge_df.groupby(\"user_label\")[\"mode\"].apply(list).to_dict()\n",
    "        self.user2score = merge_df.groupby(\"user_label\")[\"score\"].apply(list).to_dict()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.merge_df[\"user_label\"].nunique()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        出力したいもの\n",
    "        - input_tensor: user_id, anime_id 系列　を結合したもの\n",
    "        - mode_tensor: user_idか、train用の anime_id か、validation用のanime_idか、test用のanime_id かを判断するためのもの。\n",
    "        損失計算の対象を決めるために設定する。{user_id: 0, train:1, valid:2, test:3}\n",
    "        - attention_mask: 計算対象外のpaddingの位置をtransformerに教えるために必要\n",
    "        - score_tensor: ラベルとなるスコア情報。ラベルが無いものは適当に0で埋めるが使わない\n",
    "        \"\"\"\n",
    "        user_tensor = torch.Tensor([idx]).int()\n",
    "        anime_tensor = torch.Tensor(self.user2anime_dict[idx]).int()\n",
    "        mode_tensor = torch.Tensor(self.user2mode_dict[idx]).int()\n",
    "        score_tensor = torch.Tensor(self.user2score[idx]).float()\n",
    "\n",
    "        # ランダムに順序を変更する\n",
    "        indices = torch.randperm(anime_tensor.size(0))\n",
    "        anime_tensor = anime_tensor[indices]\n",
    "        mode_tensor = mode_tensor[indices]\n",
    "        score_tensor = score_tensor[indices]\n",
    "\n",
    "        pad_length = self.max_padding - anime_tensor.size(0)\n",
    "\n",
    "        # unseen用 (user_tensorは入れない）\n",
    "        attention_mask = torch.zeros([self.max_padding, self.max_padding], dtype=torch.bool)\n",
    "        attention_mask[: anime_tensor.size(0), : anime_tensor.size(0)] = True\n",
    "        input_tensor = torch.cat((anime_tensor, torch.zeros(pad_length, dtype=torch.int32)))\n",
    "        mode_tensor = torch.cat(\n",
    "            (\n",
    "                mode_tensor,\n",
    "                torch.zeros(pad_length, dtype=torch.int32),\n",
    "            )\n",
    "        )\n",
    "        score_tensor = torch.cat(\n",
    "            (\n",
    "                score_tensor,\n",
    "                torch.zeros(pad_length, dtype=torch.float),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        # seen用\n",
    "        attention_mask = torch.zeros([self.max_padding + 1, self.max_padding + 1], dtype=torch.bool)\n",
    "        attention_mask[: anime_tensor.size(0) + 1, : anime_tensor.size(0) + 1] = True\n",
    "        input_tensor = torch.cat((user_tensor, anime_tensor, torch.zeros(pad_length, dtype=torch.int32)))\n",
    "        mode_tensor = torch.cat(\n",
    "            (\n",
    "                torch.zeros(1, dtype=torch.int32),\n",
    "                mode_tensor,\n",
    "                torch.zeros(pad_length, dtype=torch.int32),\n",
    "            )\n",
    "        )\n",
    "        score_tensor = torch.cat(\n",
    "            (\n",
    "                torch.zeros(1, dtype=torch.float),\n",
    "                score_tensor,\n",
    "                torch.zeros(pad_length, dtype=torch.float),\n",
    "            )\n",
    "        )\n",
    "        \"\"\"\n",
    "        sample = {\n",
    "            \"user_ids\": user_tensor,\n",
    "            \"input_tensor\": input_tensor,\n",
    "            \"mode_tensor\": mode_tensor,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"score_tensor\": score_tensor,\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers=2,\n",
    "        hidden_size: int = 64,\n",
    "        nhead: int = 4,\n",
    "        dim_feedforward: int = 1024,\n",
    "    ):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # embedding\n",
    "        self.user_embedding = nn.Embedding(2000, hidden_size)\n",
    "        self.anime_embedding = nn.Embedding(2000, hidden_size)\n",
    "\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_size,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=0.0,\n",
    "                batch_first=True,\n",
    "            ),\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_size, hidden_size), nn.ReLU(), nn.Linear(hidden_size, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.anime_embedding(x[:, :])\n",
    "        \"\"\" seen用\n",
    "        user_x = self.user_embedding(x[:, 0:1])\n",
    "        anime_x = self.anime_embedding(x[:, 1:])\n",
    "        x = torch.cat([user_x, anime_x], dim=1)\n",
    "        \"\"\"\n",
    "        x = self.transformer_encoder(x)\n",
    "        output = self.fc(x).squeeze(2)\n",
    "        return output\n",
    "\n",
    "    def get_losses(\n",
    "        self,\n",
    "        input: torch.Tensor,\n",
    "        target: torch.Tensor,\n",
    "        mode_tensor: torch.Tensor,\n",
    "        mode: int = 1,\n",
    "    ) -> float:\n",
    "        loss_fn = nn.MSELoss()\n",
    "        loss = loss_fn(input[mode_tensor == mode], target[mode_tensor == mode])\n",
    "        loss = torch.sqrt(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0e71d87-93f5-4d7c-9adc-6ec82317462c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed_everything(config.seed)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "output_path = Path(f\".\")\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6ee8420-fe3d-446e-8804-50adbd6b27fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(Path(config.input_path) / \"train.csv\")\n",
    "test_df = pd.read_csv(Path(config.input_path) / \"test.csv\")\n",
    "anime = pd.read_csv(Path(config.input_path) / \"anime.csv\")\n",
    "train_user_ids = load_target(\"user_id\")\n",
    "sub = load_sample_sub()\n",
    "\n",
    "if config.debug:\n",
    "    n = 100\n",
    "    sample_index = train_df.sample(n).index\n",
    "    train_df = train_df.iloc[sample_index].reset_index(drop=True)\n",
    "    test_df = test_df.head(n)\n",
    "    train_user_ids = train_user_ids.iloc[sample_index].reset_index(drop=True)\n",
    "    sub = sub.head(n)\n",
    "\n",
    "\n",
    "# Merge the train data with the anime meta data\n",
    "all_df = pd.concat([train_df, test_df]).reset_index(drop=True)\n",
    "\n",
    "# 0-indexedの連番にする\n",
    "all_df[\"user_label\"], user_idx = pd.factorize(all_df[\"user_id\"])\n",
    "all_df[\"anime_label\"], anime_idx = pd.factorize(all_df[\"anime_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d669efd6-aaa7-43fc-9394-a19b3c045e68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 start !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:909: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a4fb67a4aeb449e942106421de337d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d30cfdf32f5471bb772743f4f2a2a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Train loss 8.052 Val loss: 7.821\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8631a2963d244325b093b924088c3ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 start !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fab4616af404d7586fbf50ed2a60c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c39fed9abcb407ab3c2870d98bb1857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Train loss 8.147 Val loss: 7.449\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e36029c0bdf4642ad5858688bc1ea59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 start !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70218be9f67340859d76ee74fdbb6512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efeffc09d5e43149545e51337785398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Train loss 8.062 Val loss: 7.605\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4420600a524dfeb2d76df29f53b3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 start !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95990c1fdc874ac38f863ac72f3775f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41608a9c41741b0834485d5ff951a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Train loss 7.925 Val loss: 8.358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6eb90046c9441ba5cb39f4d19446bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 start !\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbda0b8a4b75478895e86f68da999d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b795c900a89946ceabf3043d9fc7af6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Train loss 8.288 Val loss: 8.618\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e717f3cc7845e89a5fc135cd7851fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    cfg,\n",
    "    count_steps,\n",
    "    current_epoch: int,\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    n_iter: int,\n",
    "    scaler: GradScaler,\n",
    "):\n",
    "    start_time = time.time()\n",
    "    progress_bar = tqdm(dataloader, dynamic_ncols=True)\n",
    "\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    meters = {\"loss_avg\": AverageMeter()}\n",
    "    for step, data in enumerate(progress_bar):\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "\n",
    "        with autocast(enabled=cfg.use_amp):\n",
    "            output = model(data[\"input_tensor\"], data[\"attention_mask\"])\n",
    "            loss = model.get_losses(output, data[\"score_tensor\"], data[\"mode_tensor\"], 1)\n",
    "\n",
    "        if cfg.accumulation_steps > 1:\n",
    "            loss_bw = loss / cfg.accumulation_steps\n",
    "            scaler.scale(loss_bw).backward()\n",
    "            if cfg.clip_grad_norm is not None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
    "            if (step + 1) % cfg.accumulation_steps == 0 or step == n_iter:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step(count_steps)\n",
    "                optimizer.zero_grad()\n",
    "                count_steps += 1\n",
    "        else:\n",
    "            scaler.scale(loss).backward()\n",
    "            if cfg.clip_grad_norm is not None:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step(count_steps)\n",
    "            optimizer.zero_grad()\n",
    "            count_steps += 1\n",
    "\n",
    "        meters[\"loss_avg\"].update(loss.item(), dataloader.batch_size)\n",
    "        progress_bar.set_description(\n",
    "            f\"train: loss(step): {loss.item():.5f}\"\n",
    "            + f\" loss(avg): {meters['loss_avg'].avg:.5f}\"\n",
    "            + f\" lr: {optimizer.param_groups[0]['lr']:.6f}\"\n",
    "        )\n",
    "\n",
    "    return (meters[\"loss_avg\"].avg, (time.time() - start_time) / 60)\n",
    "\n",
    "\n",
    "def val_one_epoch(\n",
    "    cfg,\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    ") -> Tuple[np.ndarray, float, float]:\n",
    "    start_time = time.time()\n",
    "    progress_bar = tqdm(dataloader, dynamic_ncols=True)\n",
    "\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    meters = {\"loss_avg\": AverageMeter()}\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for step, data in enumerate(progress_bar):\n",
    "            for k, v in data.items():\n",
    "                data[k] = v.to(device)\n",
    "            output = model(data[\"input_tensor\"], data[\"attention_mask\"])\n",
    "            if (data[\"mode_tensor\"] == 2).sum() > 0:\n",
    "                loss = model.get_losses(output, data[\"score_tensor\"], data[\"mode_tensor\"], 2)\n",
    "                preds.append(output.detach().cpu().numpy())\n",
    "                meters[\"loss_avg\"].update(loss.item(), dataloader.batch_size)\n",
    "                progress_bar.set_description(\n",
    "                    f\"  val: loss(step): {loss.item():.5f}\" + f\" loss(avg): {meters['loss_avg'].avg:.5f}\"\n",
    "                )\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    return (preds, meters[\"loss_avg\"].avg, (time.time() - start_time) / 60)\n",
    "\n",
    "\n",
    "def infer(\n",
    "    cfg,\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    ") -> Tuple[np.ndarray, float]:\n",
    "    start_time = time.time()\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    pred_dict_val = {\"user_label\": [], \"anime_label\": [], \"score\": []}\n",
    "    pred_dict = {\"user_label\": [], \"anime_label\": [], \"score\": []}\n",
    "    with torch.no_grad():\n",
    "        for step, data in enumerate(tqdm(dataloader, dynamic_ncols=True)):\n",
    "            for k, v in data.items():\n",
    "                data[k] = v.to(device)\n",
    "            output = model(data[\"input_tensor\"], data[\"attention_mask\"])\n",
    "\n",
    "            data[\"user_ids\"] = data[\"user_ids\"].detach().cpu().numpy()\n",
    "            data[\"input_tensor\"] = data[\"input_tensor\"].detach().cpu().numpy()\n",
    "            data[\"score_tensor\"] = data[\"score_tensor\"].detach().cpu().numpy()\n",
    "            data[\"mode_tensor\"] = data[\"mode_tensor\"].detach().cpu().numpy()\n",
    "            output = output.detach().cpu().numpy()\n",
    "\n",
    "            user_tensor = np.copy(data[\"input_tensor\"])\n",
    "            user_tensor[:, :] = data[\"user_ids\"][:, 0:1]  # user_id の値を突っ込む\n",
    "            \"\"\"\n",
    "            user_tensor = np.copy(data[\"input_tensor\"])\n",
    "            user_tensor[:, 1:] = user_tensor[:, 0:1]\n",
    "            \"\"\"\n",
    "\n",
    "            pred_dict_val[\"user_label\"].append(user_tensor[data[\"mode_tensor\"] == 2])\n",
    "            pred_dict_val[\"anime_label\"].append(data[\"input_tensor\"][data[\"mode_tensor\"] == 2])\n",
    "            pred_dict_val[\"score\"].append(output[data[\"mode_tensor\"] == 2])\n",
    "\n",
    "            pred_dict[\"user_label\"].append(user_tensor[data[\"mode_tensor\"] == 3])\n",
    "            pred_dict[\"anime_label\"].append(data[\"input_tensor\"][data[\"mode_tensor\"] == 3])\n",
    "            pred_dict[\"score\"].append(output[data[\"mode_tensor\"] == 3])\n",
    "\n",
    "    pred_dict_val[\"user_label\"] = np.concatenate(pred_dict_val[\"user_label\"])\n",
    "    pred_dict_val[\"anime_label\"] = np.concatenate(pred_dict_val[\"anime_label\"])\n",
    "    pred_dict_val[\"score\"] = np.concatenate(pred_dict_val[\"score\"])\n",
    "    pred_dict[\"user_label\"] = np.concatenate(pred_dict[\"user_label\"])\n",
    "    pred_dict[\"anime_label\"] = np.concatenate(pred_dict[\"anime_label\"])\n",
    "    pred_dict[\"score\"] = np.concatenate(pred_dict[\"score\"])\n",
    "    return (pred_dict_val, pred_dict, (time.time() - start_time) / 60)\n",
    "\n",
    "\n",
    "all_df[\"mode\"] = 0  # 初期化\n",
    "pred_dict_list = []\n",
    "pred_dict_val_list = []\n",
    "kf = StratifiedGroupKFold(n_splits=config.tvtt.num_folds, shuffle=True, random_state=config.seed)\n",
    "for fold, (train_index, valid_index) in enumerate(kf.split(train_df, train_df[\"score\"], train_user_ids)):\n",
    "    print(f\"Fold {fold} start !\")\n",
    "\n",
    "    # ここで、trainとvalidに分ける。foldごとにデータセットを作らないとおかしくなるので注意\n",
    "    # all_df[:len(train_df)] までのデータのmodeを決定\n",
    "    all_df.loc[train_index, \"mode\"] = 1\n",
    "    all_df.loc[valid_index, \"mode\"] = 2\n",
    "    # all_df[len(train_df):] のデータのmode (test) を埋める\n",
    "    all_df.loc[len(train_df) :, \"mode\"] = 3\n",
    "\n",
    "    dataset = UserDataset(all_df)\n",
    "    model = TransformerModel(\n",
    "        hidden_size=config.tvtt.hidden_size, nhead=config.tvtt.nhead, dim_feedforward=config.tvtt.dim_feedforward\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.tvtt.batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    eval_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.tvtt.batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # other\n",
    "    best_loss = 1e9\n",
    "    num_train_iter = len(train_loader)\n",
    "\n",
    "    # optimizer & scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=float(config.tvtt.init_lr))\n",
    "\n",
    "    num_train_optimization_steps = int(len(train_loader) * config.tvtt.n_epochs // config.tvtt.accumulation_steps) + 1\n",
    "\n",
    "    scheduler = CosineLRScheduler(\n",
    "        optimizer,\n",
    "        t_initial=num_train_optimization_steps,\n",
    "        lr_min=float(config.tvtt.final_lr),\n",
    "        warmup_t=int(num_train_optimization_steps * config.tvtt.num_warmup_steps_rate),\n",
    "        warmup_lr_init=config.tvtt.warmup_lr_init,\n",
    "        warmup_prefix=True,\n",
    "    )\n",
    "    scaler = GradScaler(enabled=config.tvtt.use_amp)\n",
    "\n",
    "    # Training\n",
    "    count_steps = 0\n",
    "    early_stopping_counter = 0\n",
    "    for epoch in range(1, config.tvtt.n_epochs + 1 if config.debug is False else 2):\n",
    "        train_loss, train_time = train_one_epoch(\n",
    "            config.tvtt,\n",
    "            count_steps,\n",
    "            current_epoch=epoch,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            dataloader=train_loader,\n",
    "            n_iter=num_train_iter,\n",
    "            scaler=scaler,\n",
    "        )\n",
    "        val_preds, val_loss, val_time = val_one_epoch(cfg=config.tvtt, model=model, dataloader=eval_loader)\n",
    "        print(f\"Epoch {epoch} : Train loss {train_loss:.3f} Val loss: {val_loss:.3f}\")\n",
    "\n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(\n",
    "                {\"state_dict\": model.state_dict()},\n",
    "                output_path / \"best.pth\",\n",
    "            )\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= config.tvtt.early_stopping:\n",
    "                break\n",
    "\n",
    "    for _ in range(config.tvtt.num_tts):\n",
    "        # Infernce\n",
    "        model.load_state_dict(torch.load(output_path / \"best.pth\")[\"state_dict\"])\n",
    "        pred_dict_val, pred_dict, _ = infer(config.tvtt, model, eval_loader)\n",
    "\n",
    "        pred_dict_val_list.append(pred_dict_val)\n",
    "        pred_dict_list.append(pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9447b42c-a9e1-4b59-8dd8-4a7f04d7828b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# pred_dict_list と pred_dict_val_list を元にデータフレームを作り、user_id, anime_id でgroupbyしてからleftjoinする\n",
    "\n",
    "result_dict = {\"user_label\": [], \"anime_label\": [], \"score\": []}\n",
    "\n",
    "for pred_dict_val in pred_dict_val_list:\n",
    "    for key, val in pred_dict_val.items():\n",
    "        result_dict[key] += list(val)\n",
    "\n",
    "for pred_dict in pred_dict_list:\n",
    "    for key, val in pred_dict.items():\n",
    "        result_dict[key] += list(val)\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(result_dict)\n",
    "result_df = result_df.groupby([\"user_label\", \"anime_label\"], as_index=False).mean()\n",
    "\n",
    "\n",
    "oof_df = all_df[[\"user_label\", \"anime_label\"]].iloc[: len(train_df)].copy()\n",
    "test_preds_df = all_df[[\"user_label\", \"anime_label\"]].iloc[len(train_df) :].copy()\n",
    "\n",
    "oof_df = oof_df.merge(result_df, on=[\"user_label\", \"anime_label\"], how=\"left\")\n",
    "print(oof_df[\"score\"].isnull().sum())\n",
    "test_preds_df = test_preds_df.merge(result_df, on=[\"user_label\", \"anime_label\"], how=\"left\")\n",
    "print(test_preds_df[\"score\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ae06df52-a235-4105-8dde-7a70fa14e807",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "oof_pred = oof_df[\"score\"].to_numpy()\n",
    "mean_y_preds = test_preds_df[\"score\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3c11e840-67dc-4180-9d0c-472dc5270f06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===CV scores===\n",
      "{'nn/rmse/all_val': 7.044146506142529}\n",
      "saved: .\n"
     ]
    }
   ],
   "source": [
    "# 範囲内にする\n",
    "oof_pred = np.clip(oof_pred, 1.0, 10.0)\n",
    "mean_y_preds = np.clip(mean_y_preds, 1.0, 10.0)\n",
    "\n",
    "# CVスコア確認\n",
    "print(\"===CV scores===\")\n",
    "rmse_all_valid = evaluate_score(train_df[\"score\"], oof_pred, \"rmse\")\n",
    "print({\"nn/rmse/all_val\": rmse_all_valid})\n",
    "# wandb.log({\"nn/rmse/all_val\": rmse_all_valid})\n",
    "\n",
    "# 保存\n",
    "oof_df = pd.DataFrame({\"score\": oof_pred})\n",
    "oof_df.to_csv(output_path / \"oof.csv\", index=False)\n",
    "\n",
    "sub[config.nn.target_name] = mean_y_preds\n",
    "sub.to_csv(output_path / \"sub.csv\", index=False)\n",
    "\n",
    "print(f\"saved: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
